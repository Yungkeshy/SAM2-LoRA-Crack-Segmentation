# -*- coding: utf-8 -*-
"""Using SAM2 + LoRA on Crack Segmentation Dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nRHpX8ASViVav5FoEOkHeZ-I5NCSJ2H9
"""

# Commented out IPython magic to ensure Python compatibility.
# ─────────────────────────────────────────────────────────────────────────────
# 0. Verify GPU Runtime (make sure “GPU” is selected in Runtime → Change runtime type)
# ─────────────────────────────────────────────────────────────────────────────
!nvidia-smi

# ─────────────────────────────────────────────────────────────────────────────
# 1. Install Required Packages
# ─────────────────────────────────────────────────────────────────────────────
!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
!pip install -q kaggle opencv-python matplotlib tqdm loralib huggingface_hub

# Clone SAM-2 and install in editable mode
!git clone https://github.com/facebookresearch/sam2.git
# %cd sam2
!pip install -q -e .
# %cd ..

# ─────────────────────────────────────────────────────────────────────────────
# 2. Upload Kaggle API Key (kaggle.json)
# ─────────────────────────────────────────────────────────────────────────────
from google.colab import files
uploaded = files.upload()  # select your kaggle.json

import os
os.makedirs('/root/.kaggle', exist_ok=True)
!mv kaggle.json /root/.kaggle/
!chmod 600 /root/.kaggle/kaggle.json

# ─────────────────────────────────────────────────────────────────────────────
# 3. Download & Unzip Crack Segmentation Dataset via Kaggle API
# ─────────────────────────────────────────────────────────────────────────────
# %cd /content
from kaggle.api.kaggle_api_extended import KaggleApi
api = KaggleApi()
api.authenticate()

api.dataset_download_files(
    'lakshaymiddha/crack-segmentation-dataset',
    path='.', unzip=True
)
!find /content/crack_segmentation_dataset -maxdepth 2 -type d

# ─────────────────────────────────────────────────────────────────────────────
# 4. Download SAM-2 “ViT-B” (hiera_base_plus) Checkpoint via Hugging Face Hub
# ─────────────────────────────────────────────────────────────────────────────
from huggingface_hub import hf_hub_download

checkpoint_path = hf_hub_download(
    repo_id    = "facebook/sam2-hiera-base-plus",
    filename   = "sam2_hiera_base_plus.pt",
    local_dir  = "/content/sam2/checkpoints"
)
print("Downloaded checkpoint to:", checkpoint_path)

# ─────────────────────────────────────────────────────────────────────────────
# 5. Change Directory into the SAM-2 Repo (to avoid “parent folder” shadowing)
# ─────────────────────────────────────────────────────────────────────────────
# %cd /content/sam2

# ─────────────────────────────────────────────────────────────────────────────
# 6. Imports & Device Setup (now that cwd=/content/sam2)
# ─────────────────────────────────────────────────────────────────────────────
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
from torchvision import transforms
from PIL import Image
from tqdm import tqdm
import matplotlib.pyplot as plt

from sam2.build_sam import build_sam2

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

# ─────────────────────────────────────────────────────────────────────────────
# 7. Define CrackDataset Class
# ─────────────────────────────────────────────────────────────────────────────
import os

class CrackDataset(Dataset):
    """
    Loads image/mask pairs from images_dir & masks_dir.
    Applies Resize(512×512) + ToTensor to both.
    Binarizes mask (0 or 1).
    """
    def __init__(self, image_dir, mask_dir, transform=None):
        self.images = sorted(os.listdir(image_dir))
        self.masks  = sorted(os.listdir(mask_dir))
        self.image_dir = image_dir
        self.mask_dir  = mask_dir
        self.transform = transform

        assert len(self.images) == len(self.masks), "Image/mask counts must match"
        assert all(
            os.path.splitext(self.images[i])[0] == os.path.splitext(self.masks[i])[0]
            for i in range(len(self.images))
        ), "Image and mask filenames do not align"

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        img_path  = os.path.join(self.image_dir, self.images[idx])
        mask_path = os.path.join(self.mask_dir,  self.masks[idx])

        image = Image.open(img_path).convert("RGB")
        mask  = Image.open(mask_path).convert("L")

        if self.transform:
            image = self.transform(image)  # [3,512,512]
            mask  = self.transform(mask)   # [1,512,512]

        mask = torch.where(mask > 0.5, 1.0, 0.0)
        return image, mask

# ─────────────────────────────────────────────────────────────────────────────
# 8. Transforms & DataLoaders (Resize to 512×512, 80/20 split)
# ─────────────────────────────────────────────────────────────────────────────
transform = transforms.Compose([
    transforms.Resize((512, 512)),
    transforms.ToTensor(),
])

images_dir = "/content/crack_segmentation_dataset/images"
masks_dir  = "/content/crack_segmentation_dataset/masks"

full_dataset = CrackDataset(images_dir, masks_dir, transform=transform)
total_count  = len(full_dataset)
train_count  = int(0.8 * total_count)
test_count   = total_count - train_count

train_dataset, test_dataset = random_split(full_dataset, [train_count, test_count])

train_loader = DataLoader(
    train_dataset, batch_size=4, shuffle=True,  num_workers=2, pin_memory=True
)
test_loader  = DataLoader(
    test_dataset,  batch_size=4, shuffle=False, num_workers=2, pin_memory=True
)

print(f"Train samples: {len(train_dataset)}, Test samples: {len(test_dataset)}")

# ─────────────────────────────────────────────────────────────────────────────
# 9. Build & Prepare SAM-2 Model (Load, Resize PosEmbed, Freeze)
# ─────────────────────────────────────────────────────────────────────────────
config_name     = "sam2_hiera_b+"
checkpoint_path = "/content/sam2/checkpoints/sam2_hiera_base_plus.pt"

sam2 = build_sam2(config_name, checkpoint_path).to(device)
sam2.eval()

orig_pos = sam2.image_encoder.trunk.pos_embed  # [1, 112, 14, 14]
print("Original pos_embed shape:", orig_pos.shape)

new_grid_size = 512 // 16  # =32
grid_pos = F.interpolate(orig_pos, size=(new_grid_size, new_grid_size), mode='bilinear', align_corners=False)
sam2.image_encoder.trunk.pos_embed = nn.Parameter(grid_pos.to(device))
print("Resized pos_embed shape:", sam2.image_encoder.trunk.pos_embed.shape)

for param in sam2.parameters():
    param.requires_grad = False

with torch.no_grad():
    sample_images, _ = next(iter(train_loader))
    sample_images = sample_images.to(device)
    out = sam2.image_encoder(sample_images)
    print("Encoder returned keys:")
    for k, v in out.items():
        if isinstance(v, list):
            print(f"  {k}: list of length {len(v)}, each tensor shape = {tuple(v[0].shape)}")
        else:
            print(f"  {k}: {type(v)} {tuple(v.shape)}")

# ─────────────────────────────────────────────────────────────────────────────
# 10. Define Segmentation Head, Loss, Optimizer, Metrics
# ─────────────────────────────────────────────────────────────────────────────
class SegmentationHead(nn.Module):
    def __init__(self, in_channels, out_channels=1):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels, 128, kernel_size=3, padding=1)
        self.relu  = nn.ReLU()
        self.conv2 = nn.Conv2d(128, out_channels, kernel_size=1)

    def forward(self, x):
        x = self.relu(self.conv1(x))  # [B,128,32,32]
        return self.conv2(x)          # [B,  1,32,32]

encoder_channels = 256
seg_head = SegmentationHead(encoder_channels).to(device)

criterion = nn.BCEWithLogitsLoss()
optimizer = optim.Adam(seg_head.parameters(), lr=1e-3)

def iou_score(preds, masks, threshold=0.5):
    preds_bin   = (torch.sigmoid(preds) > threshold).float()
    intersection = (preds_bin * masks).sum(dim=(1,2,3))
    union = preds_bin.sum(dim=(1,2,3)) + masks.sum(dim=(1,2,3)) - intersection
    return ((intersection + 1e-6) / (union + 1e-6)).mean().item()

def dice_coeff(preds, masks, threshold=0.5):
    preds_bin   = (torch.sigmoid(preds) > threshold).float()
    intersection = (preds_bin * masks).sum(dim=(1,2,3))
    return ((2*intersection + 1e-6) / (preds_bin.sum(dim=(1,2,3)) + masks.sum(dim=(1,2,3)) + 1e-6)).mean().item()

# ─────────────────────────────────────────────────────────────────────────────
# 11. Baseline Training & Evaluation (100 Epochs)
# ─────────────────────────────────────────────────────────────────────────────
def train_one_epoch(sam_model, seg_head, loader, optimizer, criterion, device):
    seg_head.train()
    total_loss = 0.0
    for images, masks in tqdm(loader, desc="Baseline Training"):
        images, masks = images.to(device), masks.to(device)
        optimizer.zero_grad()
        out = sam_model.image_encoder(images)
        vf = out["vision_features"]
        if isinstance(vf, dict):
            feats = next(
                v for v in vf.values()
                if (not v.is_sparse) and v.ndim == 4 and v.shape[-2:] == (32, 32)
            )
        else:
            feats = vf  # Already [B,256,32,32]
        logits_small = seg_head(feats)
        logits_full  = F.interpolate(logits_small, size=(512,512), mode='bilinear', align_corners=False)
        loss = criterion(logits_full, masks)
        loss.backward()
        optimizer.step()
        total_loss += loss.item() * images.size(0)
    return total_loss / len(loader.dataset)

def evaluate(sam_model, seg_head, loader, device):
    seg_head.eval()
    sum_iou, sum_dice, count = 0.0, 0.0, 0
    with torch.no_grad():
        for images, masks in loader:
            images, masks = images.to(device), masks.to(device)
            out = sam_model.image_encoder(images)
            vf = out["vision_features"]
            if isinstance(vf, dict):
                feats = next(
                    v for v in vf.values()
                    if (not v.is_sparse) and v.ndim == 4 and v.shape[-2:] == (32, 32)
                )
            else:
                feats = vf
            logits_small = seg_head(feats)
            logits_full  = F.interpolate(logits_small, size=(512,512), mode='bilinear', align_corners=False)
            sum_iou  += iou_score(logits_full, masks)
            sum_dice += dice_coeff(logits_full, masks)
            count += 1
    return sum_iou / count, sum_dice / count

num_epochs = 100
baseline_train_losses = []
baseline_val_ious     = []
baseline_val_dices    = []

for epoch in range(num_epochs):
    print(f"\n=== Baseline Epoch {epoch+1}/{num_epochs} ===")
    train_loss = train_one_epoch(sam2, seg_head, train_loader, optimizer, criterion, device)
    iou, dice   = evaluate(sam2, seg_head, test_loader, device)
    baseline_train_losses.append(train_loss)
    baseline_val_ious.append(iou)
    baseline_val_dices.append(dice)
    print(f"Epoch {epoch+1}: Train Loss={train_loss:.4f} | Test IoU={iou:.4f} | Test Dice={dice:.4f}")

# ─────────────────────────────────────────────────────────────────────────────
# 12. Plot Baseline Training & Validation Metrics
# ─────────────────────────────────────────────────────────────────────────────
epochs = list(range(1, num_epochs+1))
plt.figure(figsize=(6,4))
plt.plot(epochs, baseline_train_losses, marker='o', label='Train Loss (Baseline)')
plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title('Baseline: Training Loss')
plt.grid(True); plt.legend(); plt.show()

plt.figure(figsize=(6,4))
plt.plot(epochs, baseline_val_ious, marker='o', label='Test IoU (Baseline)')
plt.plot(epochs, baseline_val_dices, marker='s', label='Test Dice (Baseline)')
plt.xlabel('Epoch'); plt.ylabel('Metric'); plt.title('Baseline: Test IoU & Dice')
plt.grid(True); plt.legend(); plt.show()

# ─────────────────────────────────────────────────────────────────────────────
# 13. Baseline Qualitative Visualization on Test Images
# ─────────────────────────────────────────────────────────────────────────────
sam2.eval()
seg_head.eval()
images, masks = next(iter(test_loader))
images, masks = images.to(device), masks.to(device)
with torch.no_grad():
    out = sam2.image_encoder(images)
    vf = out["vision_features"]
    if isinstance(vf, dict):
        feats = next(
            v for v in vf.values()
            if (not v.is_sparse) and v.ndim == 4 and v.shape[-2:] == (32, 32)
        )
    else:
        feats = vf
    logits_small = seg_head(feats)
    logits_full  = F.interpolate(logits_small, size=(512,512), mode='bilinear', align_corners=False)
    preds = torch.sigmoid(logits_full).cpu()

images_cpu = images.cpu()
masks_cpu  = masks.cpu()
num_display = 3
fig, axes = plt.subplots(num_display, 3, figsize=(9, 3*num_display))
for i in range(num_display):
    img_np    = images_cpu[i].permute(1,2,0).numpy()
    gt_mask   = masks_cpu[i,0].numpy()
    pred_mask = (preds[i,0] > 0.5).float().numpy()
    axes[i,0].imshow((img_np*255).astype('uint8')); axes[i,0].set_title("Input"); axes[i,0].axis('off')
    axes[i,1].imshow(gt_mask, cmap='gray');            axes[i,1].set_title("GT Mask"); axes[i,1].axis('off')
    axes[i,2].imshow(pred_mask, cmap='gray');          axes[i,2].set_title("Pred (Baseline)"); axes[i,2].axis('off')
plt.tight_layout(); plt.show()

# ─────────────────────────────────────────────────────────────────────────────
# 14. Baseline Additional Evaluation (Pixel‐Wise Accuracy & Average IoU)
# ─────────────────────────────────────────────────────────────────────────────
def pixel_accuracy(preds, masks, threshold=0.5):
    preds_bin = (preds > threshold).float()
    correct   = (preds_bin == masks).float().sum()
    total     = torch.numel(masks)
    return (correct / total).item()

sam2.eval()
seg_head.eval()
sum_iou_full = 0.0
sum_acc_full = 0.0
count_batches= 0
with torch.no_grad():
    for images, masks in test_loader:
        images, masks = images.to(device), masks.to(device)
        out = sam2.image_encoder(images)
        vf = out["vision_features"]
        if isinstance(vf, dict):
            feats = next(
                v for v in vf.values()
                if (not v.is_sparse) and v.ndim == 4 and v.shape[-2:] == (32, 32)
            )
        else:
            feats = vf
        logits_small = seg_head(feats)
        logits_full  = F.interpolate(logits_small, size=(512,512), mode='bilinear', align_corners=False)
        preds = torch.sigmoid(logits_full)
        sum_iou_full += iou_score(logits_full, masks)
        sum_acc_full += pixel_accuracy(preds, masks)
        count_batches+= 1
avg_iou_full = sum_iou_full / count_batches
avg_acc_full = sum_acc_full / count_batches
print(f"[Baseline] Full Test Set → Avg IoU: {avg_iou_full:.4f} | Pixel Acc: {avg_acc_full:.4f}")

torch.save(seg_head.state_dict(), "best_seg_head.pth")
print("Segmentation head weights saved to: best_seg_head.pth")

# ─────────────────────────────────────────────────────────────────────────────
# 15. Re‐load SAM-2 & Inject LoRA into Known Submodules
# ─────────────────────────────────────────────────────────────────────────────
import math

# Rebuild fresh SAM-2
sam2 = build_sam2(config_name, checkpoint_path).to(device)
sam2.eval()
orig_pos = sam2.image_encoder.trunk.pos_embed
grid_pos = F.interpolate(orig_pos, size=(new_grid_size, new_grid_size), mode='bilinear', align_corners=False)
sam2.image_encoder.trunk.pos_embed = nn.Parameter(grid_pos.to(device))
for param in sam2.parameters():
    param.requires_grad = False

class LoRALinear(nn.Module):
    def __init__(self, orig_linear: nn.Linear, r: int, alpha: float, dropout_prob: float):
        super().__init__()
        self.orig = orig_linear
        self.orig.weight.requires_grad = False
        if self.orig.bias is not None:
            self.orig.bias.requires_grad = False
        self.r = r
        self.alpha = alpha
        self.scale = alpha / r
        self.dropout = nn.Dropout(dropout_prob)
        device_lin = orig_linear.weight.device
        self.lora_A = nn.Linear(self.orig.in_features, r, bias=False).to(device_lin)
        self.lora_B = nn.Linear(r, self.orig.out_features, bias=False).to(device_lin)
        nn.init.kaiming_uniform_(self.lora_A.weight, a=math.sqrt(5))
        nn.init.zeros_(self.lora_B.weight)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        orig_out = self.orig(x)
        x_lora = x.to(self.lora_A.weight.device)
        lora_out = self.lora_B(self.lora_A(x_lora))
        return orig_out + self.dropout(lora_out) * self.scale

lora_rank    = 4
lora_alpha   = 32
lora_dropout = 0.05

def inject_lora_into_module(root_module: nn.Module, r: int, alpha: float, dropout_prob: float):
    linear_names = []
    for mod_name, mod in root_module.named_modules():
        if isinstance(mod, nn.Linear):
            linear_names.append(mod_name)
    for mod_name in linear_names:
        parts = mod_name.split('.')
        parent = root_module
        for p in parts[:-1]:
            parent = getattr(parent, p)
        orig_linear = getattr(parent, parts[-1])
        lora_linear = LoRALinear(orig_linear, r=r, alpha=alpha, dropout_prob=dropout_prob)
        setattr(parent, parts[-1], lora_linear)

print(">>> Injecting LoRA into sam2.image_encoder.trunk …")
inject_lora_into_module(sam2.image_encoder.trunk, r=lora_rank, alpha=lora_alpha, dropout_prob=lora_dropout)

if hasattr(sam2, "sam_mask_decoder"):
    print(">>> Injecting LoRA into sam2.sam_mask_decoder …")
    inject_lora_into_module(sam2.sam_mask_decoder, r=lora_rank, alpha=lora_alpha, dropout_prob=lora_dropout)
else:
    print(">>> WARNING: 'sam_mask_decoder' not found. LoRA applied only to image encoder.\n")

trainable_params = []
for name, param in sam2.named_parameters():
    if param.requires_grad:
        trainable_params.append(name)
print(f"Number of trainable parameters in sam2 (LoRA adapters only): {len(trainable_params)}")
print("Examples of trainable (LoRA) parameters:")
for n in trainable_params[:10]:
    print("   ", n)

# ─────────────────────────────────────────────────────────────────────────────
# 16. Unfreeze seg_head & Collect All Trainable Parameters (LoRA + seg_head)
# ─────────────────────────────────────────────────────────────────────────────
for p in seg_head.parameters():
    p.requires_grad = True

all_trainable_params = []
for module in sam2.modules():
    if hasattr(module, 'lora_A') and hasattr(module, 'lora_B'):
        all_trainable_params.extend(module.lora_A.parameters())
        all_trainable_params.extend(module.lora_B.parameters())
all_trainable_params.extend(seg_head.parameters())
print(f"Total number of trainable parameters (LoRA + seg_head): {len(all_trainable_params)}")

optimizer_lora = optim.AdamW(all_trainable_params, lr=1e-4)

# ─────────────────────────────────────────────────────────────────────────────
# 17. Training & Evaluation Loops (LoRA + Segmentation Head, 100 Epochs)
# ─────────────────────────────────────────────────────────────────────────────
def train_one_epoch_lora(sam_model, seg_head, loader, optimizer, criterion, device):
    seg_head.train()
    total_loss = 0.0
    for images, masks in tqdm(loader, desc="Training (LoRA)"):
        images, masks = images.to(device), masks.to(device)
        optimizer.zero_grad()
        out = sam_model.image_encoder(images)
        vf = out["vision_features"]
        if isinstance(vf, dict):
            feats = next(
                v for v in vf.values()
                if (not v.is_sparse) and v.ndim == 4 and v.shape[-2:] == (32, 32)
            )
        else:
            feats = vf
        logits_small = seg_head(feats)
        logits_full  = F.interpolate(logits_small, size=(512,512), mode='bilinear', align_corners=False)
        loss = criterion(logits_full, masks)
        loss.backward()
        optimizer.step()
        total_loss += loss.item() * images.size(0)
    return total_loss / len(loader.dataset)

def evaluate_lora(sam_model, seg_head, loader, device):
    seg_head.eval()
    sum_iou, sum_dice, count = 0.0, 0.0, 0
    with torch.no_grad():
        for images, masks in loader:
            images, masks = images.to(device), masks.to(device)
            out = sam_model.image_encoder(images)
            vf = out["vision_features"]
            if isinstance(vf, dict):
                feats = next(
                    v for v in vf.values()
                    if (not v.is_sparse) and v.ndim == 4 and v.shape[-2:] == (32, 32)
                )
            else:
                feats = vf
            logits_small = seg_head(feats)
            logits_full  = F.interpolate(logits_small, size=(512,512), mode='bilinear', align_corners=False)
            sum_iou  += iou_score(logits_full, masks)
            sum_dice += dice_coeff(logits_full, masks)
            count += 1
    return sum_iou / count, sum_dice / count

num_epochs_lora = 100
lora_train_losses = []
lora_val_ious     = []
lora_val_dices    = []

for epoch in range(num_epochs_lora):
    print(f"\n=== LoRA Epoch {epoch+1}/{num_epochs_lora} ===")
    train_loss = train_one_epoch_lora(sam2, seg_head, train_loader, optimizer_lora, criterion, device)
    iou, dice   = evaluate_lora(sam2, seg_head, test_loader, device)
    lora_train_losses.append(train_loss)
    lora_val_ious.append(iou)
    lora_val_dices.append(dice)
    print(f"Epoch {epoch+1}: LoRA Train Loss={train_loss:.4f} | LoRA Test IoU={iou:.4f} | LoRA Test Dice={dice:.4f}")

# ─────────────────────────────────────────────────────────────────────────────
# 18. Plot LoRA + Segmentation Head Metrics
# ─────────────────────────────────────────────────────────────────────────────
epochs_lora = list(range(1, num_epochs_lora+1))
plt.figure(figsize=(6,4))
plt.plot(epochs_lora, lora_train_losses, marker='o', label='Train Loss (LoRA)')
plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title('LoRA: Training Loss')
plt.grid(True); plt.legend(); plt.show()

plt.figure(figsize=(6,4))
plt.plot(epochs_lora, lora_val_ious, marker='o', label='Test IoU (LoRA)')
plt.plot(epochs_lora, lora_val_dices, marker='s', label='Test Dice (LoRA)')
plt.xlabel('Epoch'); plt.ylabel('Metric'); plt.title('LoRA: Test IoU & Dice')
plt.grid(True); plt.legend(); plt.show()

# ─────────────────────────────────────────────────────────────────────────────
# 19. Qualitative Visualization for LoRA + Segmentation Head
# ─────────────────────────────────────────────────────────────────────────────
sam2.eval()
seg_head.eval()
images, masks = next(iter(test_loader))
images, masks = images.to(device), masks.to(device)
with torch.no_grad():
    out = sam2.image_encoder(images)
    vf = out["vision_features"]
    if isinstance(vf, dict):
        feats = next(
            v for v in vf.values()
            if (not v.is_sparse) and v.ndim == 4 and v.shape[-2:] == (32, 32)
        )
    else:
        feats = vf
    logits_small = seg_head(feats)
    logits_full  = F.interpolate(logits_small, size=(512,512), mode='bilinear', align_corners=False)
    preds = torch.sigmoid(logits_full).cpu()

images_cpu = images.cpu()
masks_cpu  = masks.cpu()
num_display = 3
fig, axes = plt.subplots(num_display, 3, figsize=(9, 3*num_display))
for i in range(num_display):
    img_np    = images_cpu[i].permute(1,2,0).numpy()
    gt_mask   = masks_cpu[i,0].numpy()
    pred_mask = (preds[i,0] > 0.5).float().numpy()
    axes[i,0].imshow((img_np*255).astype('uint8')); axes[i,0].set_title("Input"); axes[i,0].axis('off')
    axes[i,1].imshow(gt_mask, cmap='gray');            axes[i,1].set_title("GT Mask"); axes[i,1].axis('off')
    axes[i,2].imshow(pred_mask, cmap='gray');          axes[i,2].set_title("Pred (LoRA)"); axes[i,2].axis('off')
plt.tight_layout(); plt.show()

# ─────────────────────────────────────────────────────────────────────────────
# 20. Additional LoRA Evaluation (Pixel‐Wise Accuracy & Avg IoU)
# ─────────────────────────────────────────────────────────────────────────────
sam2.eval()
seg_head.eval()
sum_iou_lora = 0.0
sum_acc_lora = 0.0
count_lora   = 0
with torch.no_grad():
    for images, masks in test_loader:
        images, masks = images.to(device), masks.to(device)
        out = sam2.image_encoder(images)
        vf = out["vision_features"]
        if isinstance(vf, dict):
            feats = next(
                v for v in vf.values()
                if (not v.is_sparse) and v.ndim == 4 and v.shape[-2:] == (32, 32)
            )
        else:
            feats = vf
        logits_small = seg_head(feats)
        logits_full  = F.interpolate(logits_small, size=(512,512), mode='bilinear', align_corners=False)
        preds = torch.sigmoid(logits_full)
        sum_iou_lora += iou_score(logits_full, masks)
        sum_acc_lora += pixel_accuracy(preds, masks)
        count_lora   += 1
avg_iou_lora = sum_iou_lora / count_lora
avg_acc_lora = sum_acc_lora / count_lora
print(f"[LoRA] Full Test Set → Avg IoU: {avg_iou_lora:.4f} | Pixel Acc: {avg_acc_lora:.4f}")

# ─────────────────────────────────────────────────────────────────────────────
# 21. Save Final LoRA + Segmentation Head Weights
# ─────────────────────────────────────────────────────────────────────────────
save_dict = {
    'lora_state_dict': {},
    'seg_head_state_dict': seg_head.state_dict()
}
for name, module in sam2.named_modules():
    if hasattr(module, 'lora_A') and hasattr(module, 'lora_B'):
        save_dict['lora_state_dict'][name] = {
            'lora_A': module.lora_A.state_dict(),
            'lora_B': module.lora_B.state_dict(),
            'scale': module.scale
        }
torch.save(save_dict, "sam2_lora_seghead_checkpoint.pth")
print("LoRA + Segmentation Head checkpoint saved to: sam2_lora_seghead_checkpoint.pth")

# ─────────────────────────────────────────────────────────────────────────────
# 22. Final Numeric Comparison: Baseline vs. LoRA
# ─────────────────────────────────────────────────────────────────────────────
print("\n=== Final Comparison: Baseline vs. LoRA ===")
print(f"Baseline IoU (epoch {num_epochs}): {baseline_val_ious[-1]:.4f} | LoRA IoU (epoch {num_epochs_lora}): {lora_val_ious[-1]:.4f}")
print(f"Baseline Dice (epoch {num_epochs}): {baseline_val_dices[-1]:.4f} | LoRA Dice (epoch {num_epochs_lora}): {lora_val_dices[-1]:.4f}")
print(f"Baseline Pixel Acc: {avg_acc_full:.4f} | LoRA Pixel Acc: {avg_acc_lora:.4f}")

from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, accuracy_score
import numpy as np
import torch.nn.functional as F
import torch
import pandas as pd

# --- utility from before ---
def evaluate_segmentation(model, seg_head, loader, device, threshold=0.5):
    model.eval(); seg_head.eval()
    all_preds, all_masks = [], []
    with torch.no_grad():
        for images, masks in loader:
            images = images.to(device)
            masks  = masks.to(device)

            out = model.image_encoder(images)
            vf  = out["vision_features"]
            if isinstance(vf, dict):
                feats = next(
                    v for v in vf.values()
                    if (not v.is_sparse) and v.ndim==4 and v.shape[-2:]==(32,32)
                )
            else:
                feats = vf

            logits_small = seg_head(feats)
            logits_full  = F.interpolate(
                logits_small, size=(512,512),
                mode='bilinear', align_corners=False
            )
            probs = torch.sigmoid(logits_full)

            preds = (probs > threshold).view(-1).cpu().numpy()
            gt    = masks.view(-1).cpu().numpy()

            all_preds.append(preds)
            all_masks.append(gt)

    all_preds = np.concatenate(all_preds)
    all_masks = np.concatenate(all_masks)

    tn, fp, fn, tp = confusion_matrix(all_masks, all_preds, labels=[0,1]).ravel()
    acc     = accuracy_score(all_masks, all_preds)
    prec, rec, f1, sup = precision_recall_fscore_support(
        all_masks, all_preds, labels=[1], average=None
    )
    # convert arrays to scalars
    precision, recall, f1 = prec[0], rec[0], f1[0]
    intersection = tp
    union        = tp + fp + fn
    mean_iou     = intersection / union if union>0 else 0.0
    dice         = 2*intersection / (2*intersection + fp + fn) if (2*intersection+fp+fn)>0 else 0.0

    return {
        "tp": tp, "fp": fp, "fn": fn, "tn": tn,
        "accuracy": acc,
        "precision": precision,
        "recall": recall,
        "f1": f1,
        "mean_iou": mean_iou,
        "dice": dice
    }

# --- 1) Baseline: rebuild SAM2 & load baseline head ---
# (Exactly as in your Section 9 + seg_head definition)
sam2_base = build_sam2(config_name, checkpoint_path).to(device)
# resize pos_embed + freeze ...
orig_pos = sam2_base.image_encoder.trunk.pos_embed
grid_pos = F.interpolate(orig_pos, size=(32,32), mode='bilinear', align_corners=False)
sam2_base.image_encoder.trunk.pos_embed = torch.nn.Parameter(grid_pos.to(device))
for p in sam2_base.parameters(): p.requires_grad=False

# load your saved baseline head
baseline_head = SegmentationHead(encoder_channels).to(device)
baseline_head.load_state_dict(torch.load("best_seg_head.pth"))

# evaluate
metrics_base = evaluate_segmentation(sam2_base, baseline_head, test_loader, device)

# --- 2) LoRA: rebuild SAM2 + inject LoRA adapters + load its head ---
sam2_lora = build_sam2(config_name, checkpoint_path).to(device)
# resize + freeze
orig_pos = sam2_lora.image_encoder.trunk.pos_embed
grid_pos = F.interpolate(orig_pos, size=(32,32), mode='bilinear', align_corners=False)
sam2_lora.image_encoder.trunk.pos_embed = torch.nn.Parameter(grid_pos.to(device))
for p in sam2_lora.parameters(): p.requires_grad=False

# inject LoRA adapters exactly as in Section 15
inject_lora_into_module(sam2_lora.image_encoder.trunk,    r=lora_rank, alpha=lora_alpha, dropout_prob=lora_dropout)
if hasattr(sam2_lora, "sam_mask_decoder"):
    inject_lora_into_module(sam2_lora.sam_mask_decoder, r=lora_rank, alpha=lora_alpha, dropout_prob=lora_dropout)

# load LoRA+head checkpoint
ckpt = torch.load("sam2_lora_seghead_checkpoint.pth")
for name, module in sam2_lora.named_modules():
    if hasattr(module, 'lora_A') and hasattr(module, 'lora_B'):
        module.lora_A.load_state_dict( ckpt['lora_state_dict'][name]['lora_A'] )
        module.lora_B.load_state_dict( ckpt['lora_state_dict'][name]['lora_B'] )
        module.scale = ckpt['lora_state_dict'][name]['scale']

lora_head = SegmentationHead(encoder_channels).to(device)
lora_head.load_state_dict( ckpt['seg_head_state_dict'] )

# evaluate
metrics_lora = evaluate_segmentation(sam2_lora, lora_head, test_loader, device)

# --- 3) Compare in a DataFrame ---
df = pd.DataFrame([metrics_base, metrics_lora], index=["Baseline","LoRA"])
print(df[[
    "tp","fp","fn","tn","accuracy",
    "precision","recall","f1","mean_iou","dice"
]])

